---
title: "ปกป้องเซอร์วิซด้วย Throttling"
date: 2017-11-10T12:04:02+07:00
lastmod: 2017-11-10T12:04:02+07:00
draft: true
tags: ["reliability", "design", "bulkhead"]
categories: ["reliability"]
---


![Photo by Ksenia Kudelkina, from Unsplash.com](/img/covers/limit-01.jpg)

ลองนึกภาพว่าเรามีระบบที่ถูกออแบบเป็น Service-Oriented Architecture (SOA)

ในบรรดาเซอร์วิซทั้งหมด จะบางเซอร์วิซที่สำคัญมากๆเพราะเป็น Dependency ของเซอร์วิซอื่นๆ ผมจะเรียกเซอร์วิซนี้ว่าเซอร์วิซ A

ซึ่งหากเซอร์วิซ A พังขึ้นมา เซอร์วิซอื่นที่ต้องเรียกใช้ข้อมูลนี้ทั้งหมด ก็จะทำงานไม่ได้ ดังตัวอย่างในภาพข้างล่าง

<!--more-->

###

จากภาพ หากเซอร์วิซ A พัง  เซอร์วิซ B, และ C จะทำงานไม่ได้ เพราะต้องเรียกใช้เซอร์วิซ A

ยกตัวอย่างให้เห็นภาพชัดเจน ลองนึกถึงระบบในสายการบิน

เซอร์วิซ A คือระบบที่เก็บข้อมูลเที่ยวบินทั้งหมด

* B เป็นเซอร์วิซที่สำหรับใช้เช็คอินผู้โดยสาร ถูกใช้โดยพนักงานในสนามบิน (ตรงที่เราต้องเอากระเป๋าไปโหลด)
* C เป็นเซอร์วิซที่ใช้คำนวนค่าจ้างของแอร์โฮสเตจ ซึ่งได้เงินแปรผันตามจำนวนและระยะทางของเที่ยวบิน

วันดีคืนดี มีคนเขียนโปรแกรมในระบบ C ผิดพลาด ใส่ Infinite Loop เข้าไป ทำให้พยายามยิง Request เพื่อดึงข้อมูลจากระบบ A รัวๆไม่หยุด

> อันนี้เกิดขึ้นจริงบ่อยนะครับ ผมเคยเขียนสคริปต์ผิดแนวนี้ พยายามจะส่งเมลล์ออกไปแสนกว่าฉบับ

ผลคือระบบ A รับการทำงานไม่ไหว Request อื่นๆที่ส่งมาก็จะโดน Time Out Error ตลอด

ประเด็นคือ เซอร์วิซ B ซึ่งไม่รู้อิโหน่อิเหน่อะไรด้วยก็ติดร่างแหไปด้วย ทั้งๆที่ออกแบบมาอย่างดี เทสต์มาอย่างดีทุกอย่าง ดันมาตายน้ำตื้นเพราะดันมีอีกเซอร์วิซเขียน Infinite Loop ซะงั้น

คำถามคือ เราจะป้องกันปัญหานี้ยังไงดี


# ทำความรู้จักกับ Throttling หรือ Rate Limiting

ในกรณีที่ผู้ใช้บางคนเรียกใช้เซอร์วิซเป็นจำนวนมากในระยะเวลาสั้นๆ เป็นหน้าที่ของระบบ A ที่จะต้องทำการปกป้องทรัพยากร (Resource) ของตน ไม่ให้ถูกใช้จนหมดผ่านผู้ใช้คนเดียว

คำว่าผู้ใช้ในที่นี้ อาจจะเป็นเซอร์วิซอื่นๆที่ในบริษัทเรา เหมือนกรณีข้างต้น

หรือที่อันตรายกว่า อาจจะเป็นผู้ใช้ที่เราไม่มีสิทธิ์ควบคุมได้เลย ตัวอย่างเช่น เราเปิด API ให้ Developer ทั่วไปสามารถดึงข้อมูลเที่ยวบินได้ บริษัทเอเจนซี่อื่นๆอาจทำเว็บไซต์จองตั๋ว ที่ต้องดึงข้อมูลเที่ยวบินจากเรา

ในกรณีนี้ ผู้ใช้บางรายอาจจะเขียนบอตเพื่อดึงข้อมูลรัวๆ ความผิดพลาดนั้นอาจเกิดขึ้นโดยไม่ได้ตั้งใจ หรืออาจจะเป็นการจงใจจากบริษัทคู่แข่ง เพื่อทำให้ระบบเราใช้งานไม่ได้

ดังนั้น เซอร์วิซที่ดี จะต้องมีการจำกัดความถี่ในการเรียกใช้งาน ซึ่งการจำกัดความถี่นี้เอง ที่ถูกเรียกว่า **Throttling** (บางที่ก็จะเรียกว่า **Rate Limiting**)

ตัวอย่างเช่น เราอาจจะกำหนดให้ผู้ใช้แต่ละคน สามารถส่ง Request ได้ไม่เกิน 100 ครั้งต่อวินาที หากมากเกินกว่านั้น เราจะทำการปฏิเสธ Request ถัดๆไปด้วยการส่ง Error 429 (Too Many Requests) กลับไป

# ความซับซ้อนในการกำหนด Throttling

ในทางปฏิบัติ เราควรจะใช้ไลบรารี่ที่คนอื่นทำไว้แล้ว เพราะการเขียนและทดสอบ Throttling นี่ยากมาก อย่าเสียเวลาไปเขียนเองนะครับ บั้กกระจายแน่นอน

พอใช้ไลบรารี่ นอกจากค่า rate แล้ว เราเลือกค่า Key ในที่ใช้ในการ Throttle ในที่นี้คืออะไร

วิธีที่ง่ายที่สุดคือให้ Key ตาม Username ของผู้ใช้ ตัวอย่างเช่น

Key | จำนวน Request ในช่วงวินาทีที่ผ่านมา
--------|------
user1 | 57
user2 | 30
user3 | 100

ถ้าเรากำหนด Limit ไว้ที่ 100 Requests/วินาที  ณ จุดนี้ user3 จะโดนปฏิเสธ Request ถัดๆไป

ในบางกรณี เราจะไม่รู้ว่า Username อันนี้ก็ต้องหา Unique ID อื่นๆมาใช้เป็น Key แทน เช่น Access Token

แต่บางครั้ง Request ที่ส่งมาก็ไม่รู้ว่ามาจาก User ไหน ยกตัวอย่างเช่น ระบบ B อาจจะเป็นเว็บไซต์ เวลาได้ Request ที่ต้องการดึงข้อมูลจากระบบ A คำสั่งก็จะมาในนามของระบบ B ตรงๆ โดยที่ A ไม่มีข้อมูลเลยว่ามาจาก User คนไหน เพราะข้อมูล User ถูกเก็บในระบบ

กรณีนี้ก็ต้องใช้เป็น Access Token ของระบบนั้นๆไป โดยอาจปรับเรทให้สูงหน่อย เพราะอาจจะมาจาก User หลายๆคน

ถ้าคิดว่าจะใช้ IP ก็ต้องยอมรับว่ามันเปลี่ยนได้ ผลการ Throttling จึงอาจไม่ได้เป้ะ  ยิ่งถ้าเซอร์วิซ B อาจจะมีหลายเซอร์เวอร์ Request อาจจะมาจากหลาย IP ได้

> ถ้าโอเคกับ IP ตัว Web Server หรือ Load Balancer น่าจะทำได้อยู่แล้ว แค่แก้ Config เอา ลองเช็คดูก่อนนะครับ

พอถึงจุดนี้ เราจะค้นพบว่าเซอร์วิซ B และ C อาจจะต้องยิง Request ในอัตราที่ต่างกัน ดังนั้น เราจะ Throttle ด้วยค่าเดียวกันไม่ได้

หรือในกลุ่ม User บางคน เรามีข้อตกลงพิเศษที่จะให้ดึงข้อมูลได้ปริมาณเยอะกว่าคนอื่น

เราจึงต้องกำหนด Rate ที่ต่างกันสำหรับแต่ละคีย์

# สร้าง Config file สำหรับ Throttling

พอเราเริ่มมี Throttling rate ที่แตกต่างกันไปตาม Key เราควรจะเก็บ Config ไว้ในไฟล์แยกจากโค้ด

ยกตัวอย่าง ในรูปแบบ JSON เราอาจเก็บเป็นแบบนี้

```json
ratePerSecond = {
  serviceB: 100, // key = serviceB, 100 request/second
  serviceC: 50,
  other: 10
}
```

โดยเราอาจจะให้แอพพลิเคชั่นของเราโหลด Config File ตอนเริ่มต้น Application แล้วเก็บไว้ใน Memory ตลอดการทำงาน

หรืออาจจะทำการโหลดซ้ำทุกๆนาที หากเราต้องมีการเปลี่ยนค่านี้บ่อยๆ จะได้ไม่ต้อง Restart Application ทุกครั้งที่เปลี่ยน

ถ้าเอาให้ละเอียดลงไปอีก เราอาจจะต้องใส่ชนิดของ Operation ลงไปใน Key ด้วย เช่น A อาจจะอ่านข้อมูลได้เยอะ แต่เขียนข้อมูลได้ช้ามาก เราก็ต้องอาจจะกำหนดให้การเขียนทำได้แค่ 1 ใน 10 ของการอ่าน

ตัว Config File ก็จะเป็นดังนี้

```json
ratePerSecond = {
  serviceB_read: 100,
  serviceB_write: 10,
  serviceC_read: 50,
  serviceC_write: 5,
  other_read: 10,
  other_write: 1
}
```

ตัว Key อาจจะซับซ้อนขึ้นไปอีกโดยมีมิติมากกว่าแค่ Operation เช่น อัตราจะมากขึ้นตอนกลางคืน เพราะคนใช้ไม่เยอะ อันนี้ก็แล้วแต่ออกแบบ

แต่แนะนำว่าพยายามให้ง่าย (Simple) ที่สุด จะได้ดูแลได้ง่าย ถ้าไม่จำเป็นจริงๆก็อย่าใส่อะไรเพิ่มเข้าไป แค่เซอร์วิซก็พอแล้ว


# ส่งข้อมูลของ Rate กลับไปให้ผู้ใช้
ถ้าเซอร์วิซ B มีหลายเซอร์เวอร์ บางเซอร์เวอร์อาจจะส่ง Request เยอะมาก จนชน Rate Limit ไปเรียบร้อยแล้ว

ส่วนอีกเซอร์เวอร์นึง พึ่งเรียกครั้งแรก ก็เจอะ Error 429 (Too Many Requests)

สิ่งที่เซอร์เวอร์นี้ต้องการรู้คือ ควรจะรออีกนาเท่าไร ถึงค่อยส่ง Request ถัดไป

ซึ่งข้อมูลนี้เซอร์เวอร์นี้ไม่สามารถรู้ได้ง่ายๆ เพราะต้องรู้ Rate Config ใน A  และต้องรู้ว่าเซอร์เวอร์อื่นๆของ B ส่ง Request ไปเมื่อไรบ้าง

เพื่อให้ง่ายต่อผู้ใช้ A สามารถส่งค่าพวกนี้กลับไปทาง Response ได้ โดยที่เห็นใช้กันบ่อยก็คือการใส่ค่ากลับไปใน Response Header เท่าที่เห็น API มักจะส่งค่าสามค่านี้

 * **X-Rate-Limit-Limit** : 100 (จำนวน Request ที่ส่งได้ในช่วงเวลาที่กำหนด ในกรณีนี้เป็น 100 Request/second)
 * **X-Rate-Limit-Remaining** - 45 (จำนวน Request ที่เหลือยังส่งได้ ในกรณีนี้คือส่งไปแล้ว 55 Requests  ก็ยังเหลือ 45 Request)
 * **X-Rate-Limit-Reset - 500** (เวลาก่อนที่ก่อนที่ค่า Remaining จะถูกเซ็ตกลับเป็นค่าเดิม ในกรณีนี้ อีก 500 ms, Remaining จะกลับเป็น 100)


วิธีนี้ นอกจากจะทำให้ฝั่ง B เขียนโค้ดได้ง่ายขึ้น ยังลดโอกาสที่ผู้ใช้จะยังส่ง Request มาแล้วเจอ 429 อีกรอบ เป็นผลดีต่อฝั่ง A ด้วย


# หากเซอร์วิซ A มีหลายเซอร์เวอร์
ปัญหาการ Throttling จะบานปลาย กลายเป็นปัญหา Distributed System ในทันที

สมมติว่าเรามีเซอร์เวอร์อยู่ 3 ตัว เราต้องการกำหนด Rate ให้อยู่ที่ 100 Request ต่อวินาที จะทำยังไง?

อันนี้ก็ต้องพึ่งไลบรารี่ของท่านแล้วล่ะครับ เพราะเขียนเองยากมาก อย่าลืมส่งเมลล์ไปขอบคุณคนเขียนด้วย ที่ทำให้ชีวิตง่ายขึ้น

แต่หากหาไลบรารี่ไม่ได้ อาจจะมีวิธีหลบอยู่บ้าง

1. **ทำ Throttling ตั้งแต่ก่อนกระจาย Request** โดยสร้างเซอร์เวอร์ตัวหนึ่งมากั้นกลางเหมือน Reverse Proxy แล้วทำการนับอยู่ที่เดียวแทน ถ้าผ่านถึงค่อยส่ง Request ไปต่อ
2. **เอาไปเก็บไว้ใน Database กลาง** วิธีนี้อาจจะทำได้ในกรณีที่ Request กระจายตัว ไม่ได้เข้ามาถี่มาก เพราะการอ่านจาก Database นั้นช้ากว่ามาก (แม้จะเป็น In-Memory DB)
3. **ยอมรับความคาดเคลื่อน** โดยเซ็ตค่าไปที่ 34 Request/วินาที ในแต่ละเซอร์เวอร์ ซึ่งแน่นอนว่า Request จะไม่กระจายเท่าๆกันเป๊ะ ดังนั้น Request มีโอกาสจะเข้า Server ตัวแรกบ่อยๆ และอาจจะโดนบล็อคก่อนครบ 102 Request

# Burst Rate
ในบางกรณี เราอาจจะอนุญาติให้ B เรียกใช้เซอร์วิซได้เกิน ระยะเวลาที่กำหนดในปริมาณหนึ่ง

เช่น rate อยู่ที่ 100 Requests/second  แต่อาจจะเรียก 200 Requests/second ได้ แต่หลังจากนั้นจะถูกหยุดที่ 100 แล้ว

อันนี้เราเรียก 200 ว่า Burst Rate

ฟังแล้วอาจจะสับสนว่าเป็นยังไง ลองนึกว่าเรามีถังน้ำหนึ่งใบที่รั่วๆ โดยถังจุน้ำได้ 200 ml

ทุกครั้งที่มี Request มาเราจะเอาน้ำใส่เข้าไป 1 ml ต่อ Request

และน้ำจะไหลออกจากถังด้วยอัตรา 100 ml ต่อวินาที

พอเราทำการ Burst จนถึง 200 Requests (ml) น้ำจะเต็มถัง

Request ถัดมาจะทำให้น้ำล้นออกจากถัง ซึ่งเราจะ Reject request นั้นให้เป็น 429 ไป

เนื่องจากถังรั่ว น้ำจะค่อยๆไหลออก เราจึงใส่น้ำเพิ่มไปได้ แต่เราจะ Burst ต่ออีกไม่ได้ เว้นเสียแต่ว่าจะรอให้ผ่านไป 2 วินาทีโดยไม่ส่ง Request เลย เพื่อให้น้ำไหลออกจากถังจนหมด 200 ml

# Summary
